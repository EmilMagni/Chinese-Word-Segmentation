{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MR_rwxjyeQBv"
   },
   "source": [
    "# Chinese Word Segmentation\n",
    "## Artificial Intelligence Programming\n",
    "### Emil Magni (ID: 2021400566)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJtbN2ogead8"
   },
   "source": [
    "## 1. Implementing a traditional matching algorithm\n",
    "### Forward Maximum Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['圆月', '当空', '月光如水', '世界', '一片', '朦胧']\n"
     ]
    }
   ],
   "source": [
    "# Forward Maximum matching algorithm\n",
    "# First i made this algorithm that uses the method of forward maximum matching to segment the provided sentence.\n",
    "# This algorithm requires a file with known words where there is just one word on each line.\n",
    "# It is to show an example of a simple matching without using machine learning. \n",
    "\n",
    "# the forward_matching class is defined with the constructor to create a new matching dictionary based on the file of known words.\n",
    "class forward_matching(object):\n",
    "    def __init__(self, dict_path):\n",
    "        self.dictionary = set() \n",
    "        self.maximum = 0\n",
    "        with open(dict_path, 'r', encoding='utf8') as file:\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                self.dictionary.add(line)\n",
    "                if len(line) > self.maximum:\n",
    "                    self.maximum = len(line) \n",
    "\n",
    "# the match function is used to look up the dictionary and if there is not match it will remove the last word and try again.\n",
    "    def match(self, text):\n",
    "        result = []\n",
    "        size = self.maximum\n",
    "        text_len = len(text)\n",
    "        while text_len > 0:\n",
    "            word = text[0:size]\n",
    "            while word not in self.dictionary:\n",
    "                if len(word) == 1:\n",
    "                  break\n",
    "                word = word[0:len(word) - 1]\n",
    "            result.append(word)\n",
    "            text = text[len(word):]\n",
    "            text_len = len(text)\n",
    "        return result\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = \"圆月当空月光如水世界一片朦胧\"\n",
    "    tokenizer = forward_matching('maximum_matching_dictionary.utf8')\n",
    "    print(tokenizer.match(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PGWNbjPNjnc"
   },
   "source": [
    "### Reverse Maximum Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Zm7vony4Sqpx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '爱', '吃', '火锅']\n"
     ]
    }
   ],
   "source": [
    "# Reverse Maximum matching algorithm\n",
    "class reverse_matching(object):\n",
    "    def __init__(self, dict_path):\n",
    "        self.dictionary = set()\n",
    "        self.maximum = 0\n",
    "        with open(dict_path, 'r', encoding='utf8') as file:\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                self.dictionary.add(line)\n",
    "                if len(line) > self.maximum:\n",
    "                    self.maximum = len(line)\n",
    "\n",
    "# the reverse_matching is different as it starts from the last character in the input text\n",
    "    def match(self, text):\n",
    "        result = []\n",
    "        index = len(text)\n",
    "        while index > 0:\n",
    "            word = None\n",
    "            for size in range(self.maximum, 0, -1):\n",
    "                if index - size < 0:\n",
    "                    continue\n",
    "                piece = text[(index - size):index]\n",
    "                if piece in self.dictionary:\n",
    "                    word = piece\n",
    "                    result.append(word)\n",
    "                    index -= size\n",
    "                    break\n",
    "            if word is None:\n",
    "                index -= 1\n",
    "        return result[::-1]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = \"我爱吃火锅\"\n",
    "    tokenizer = reverse_matching('maximum_matching_dictionary.utf8')\n",
    "    print(tokenizer.match(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GUI in tkinter for the reverse matching algorithm\n",
    "The GUI will need an input field where a test string can be entered\n",
    "When the segment button is pressed then the code will get the input and use it in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from tkinter.messagebox import showinfo\n",
    "\n",
    "# creating the window\n",
    "root = tk.Tk()\n",
    "root.geometry(\"300x150\")\n",
    "root.resizable(False, False)\n",
    "root.title('Word Segmentation')\n",
    "\n",
    "# First I create a string variable for storing the input text\n",
    "input_text = tk.StringVar()\n",
    "\n",
    "\n",
    "# get the text from the input field and use reverse matching before displaying the result in a message box when the segment button is clicked.\n",
    "def segment_clicked():\n",
    "    # callback for when the segment button is clicked\n",
    "   \n",
    "    text = input_text.get()\n",
    "    print(text) # printing the input below to see it in jupyter notebook\n",
    "    rm = reverse_matching('maximum_matching_dictionary.utf8')\n",
    "    print(tokenizer.match(text)) # printing the result below\n",
    "    msg = f'You entered the input: {input_text.get()} and the segmented text output is {tokenizer.match(text)}'\n",
    "    showinfo(\n",
    "        title='Result',\n",
    "        message=msg\n",
    "    )\n",
    "\n",
    "\n",
    "# input frame\n",
    "segmentation_frame = ttk.Frame(root)\n",
    "segmentation_frame.pack(padx=10, pady=10, fill='x', expand=True)\n",
    "\n",
    "\n",
    "# adding a label describing the input field\n",
    "input_label = ttk.Label(segmentation_frame, text=\"Input string:\")\n",
    "input_label.pack(fill='x', expand=True)\n",
    "\n",
    "# I create the Entry widget for the text input \n",
    "input_entry = ttk.Entry(segmentation_frame, textvariable=input_text)\n",
    "input_entry.pack(fill='x', expand=True)\n",
    "input_entry.focus()\n",
    "\n",
    "\n",
    "# segment button\n",
    "segment_button = ttk.Button(segmentation_frame, text=\"Segment text\", command=segment_clicked)\n",
    "segment_button.pack(fill='x', expand=True, pady=10)\n",
    "\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rb7bsw4fVWe"
   },
   "source": [
    "## 2. Implementing a machine learning algorithm based on the Hidden Markov Model\n",
    "Here I will create a word segmentation with the Hidden Markov Model on the given dataset. I will be using the msr datasets of simplified Chinese corpus for training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3iTeXhhkoQyV"
   },
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RxJgV6y3oPzT"
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "# Use regular expression to detect Chinese characters\n",
    "regex = re.compile(\"[^\\u4e00-\\u9fff]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WuJbuXYYfb7q"
   },
   "outputs": [],
   "source": [
    "# loading of data\n",
    "# I store the data in a list with an element for each of the sentences.\n",
    "train_data = open(os.path.join(\"icwb2-data/training\", \"msr_training.utf8\"), encoding=\"utf-8\").read().splitlines()\n",
    "test_data = open(os.path.join(\"icwb2-data/testing\", \"msr_test.utf8\"), encoding=\"utf-8\").read().splitlines()\n",
    "test_gold = open(os.path.join(\"icwb2-data/gold\", \"msr_test_gold.utf8\"), encoding=\"utf-8\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['“  人们  常  说  生活  是  一  部  教科书  ，  而  血  与  火  的  战争  更  是  不可多得  的  教科书  ，  她  确实  是  名副其实  的  ‘  我  的  大学  ’  。',\n",
       " '“  心  静  渐  知  春  似  海  ，  花  深  每  觉  影  生  香  。',\n",
       " '“  吃  屎  的  东西  ，  连  一  捆  麦  也  铡  不  动  呀  ？']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking a look at the three first sentences of the training data list.\n",
    "# as we can see the training file has the examples of segmented Chinese words separated by spaces.\n",
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['扬帆远东做与中国合作的先行', '希腊的经济结构较特殊。', '海运业雄踞全球之首，按吨位计占世界总数的１７％。']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As seen in the testing data the sentences have no segmentation\n",
    "test_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['扬帆  远东  做  与  中国  合作  的  先行  ',\n",
       " '希腊  的  经济  结构  较  特殊  。',\n",
       " '海运  业  雄踞  全球  之  首  ，  按  吨位  计  占  世界  总数  的  １７％  。']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_gold[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "The training and testing data is preprocessed according to the placement of the characters with 4 labels. The 4 labels will show where the character is placed according to the segmentation.\n",
    "<ul>\n",
    "    <li>\"S\" means it is a single character token standing alone.</li>\n",
    "    <li>\"E\" means it is the ending character of a word.</li>\n",
    "    <li>\"M\" means it is an internal character in the middle of a word.</li>\n",
    "    <li>\"B\" means the character is the beginning of a new word.</li>\n",
    "</ul>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    hidden_state_lst = []\n",
    "    data_lst = []\n",
    "    word_count = {}\n",
    "\n",
    "# the for loop goes through the sentence and labels the characters by there placement in the segmentation.\n",
    "    for sentence in data:\n",
    "        hidden_state = \"\"\n",
    "        words = []\n",
    "        for token in sentence.split():\n",
    "            token = regex.sub(\"\", token)\n",
    "            # Each character is getting a label according to the lenght of the word.\n",
    "            if len(token) == 1:\n",
    "                hidden_state += \"S\"\n",
    "            elif len(token) == 2:\n",
    "                hidden_state += \"BE\"\n",
    "            elif len(token) > 2:\n",
    "                hidden_state += \"B\" + (len(token) - 2)*\"M\" + \"E\"\n",
    "\n",
    "        # I remove the spaces\n",
    "        sentence = sentence.replace(\" \", \"\")\n",
    "        for word in sentence:\n",
    "            # regular expression to verify if the word is a Chinese character\n",
    "            word = regex.sub(\"\", word)\n",
    "            # creating a dictionary to count the words for use in calculating probabilities.\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "            words.append(word)\n",
    "        if len(words) > 0:\n",
    "            data_lst.append(words)\n",
    "            states = [s for s in hidden_state]\n",
    "            hidden_state_lst.append(states)\n",
    "\n",
    "    return data_lst, hidden_state_lst, word_count\n",
    "\n",
    "# I create dictionaries for the training data and testing data\n",
    "# the hidden states for the testing is based on the gold dataset with the correct segmentation\n",
    "# I also create a dictionary for storing of the word count (wc) for both the training and testing data.\n",
    "train_data, train_hs, train_wc = preprocess(train_data)\n",
    "test_data, _, test_wc = preprocess(test_data)\n",
    "_, test_hs, _ = preprocess(test_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B': 0.34577982777611344, 'M': 0.09436133454366076, 'E': 0.34577982777611344, 'S': 0.21407900990411233}\n"
     ]
    }
   ],
   "source": [
    "# Creating a dictionary for the state count\n",
    "state_count = {}\n",
    "states = [\"B\", \"M\", \"E\", \"S\"]\n",
    "for state in states: \n",
    "    state_count[state] = 0\n",
    "\n",
    "for i in range(len(train_hs)):\n",
    "    length = len(train_hs[i])\n",
    "    if length > 0:\n",
    "        for j in range(length - 1):\n",
    "            # Update the state count\n",
    "            state_count[train_hs[i][j]] += 1\n",
    "        state_count[train_hs[i][length - 1]] += 1\n",
    "\n",
    "total_states = sum(state_count.values())  # Getting the total number of states in the sample\n",
    "start_prob = {}\n",
    "\n",
    "for state in states:\n",
    "    # Normalize the state count with the total number of states\n",
    "    start_prob[state] = state_count[state] / total_states\n",
    "    \n",
    "print(start_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B': {'B': 0.0, 'M': 0.14804196105988887, 'E': 0.8519580389401111, 'S': 0.0}, 'M': {'B': 0.0, 'M': 0.4575116593413479, 'E': 0.542488340658652, 'S': 0.0}, 'E': {'B': 0.5755412261794932, 'M': 0.0, 'E': 0.0, 'S': 0.36684089371227374}, 'S': {'B': 0.606061801705827, 'M': 0.0, 'E': 0.0, 'S': 0.37270019136301763}}\n"
     ]
    }
   ],
   "source": [
    "# Initializing the transition probabilities\n",
    "trans_prob = {}\n",
    "for state in states:\n",
    "    trans_prob[state] = {}\n",
    "    for state_i in states:\n",
    "        trans_prob[state][state_i] = 0\n",
    "\n",
    "for i in range(len(train_hs)):\n",
    "    length = len(train_hs[i])\n",
    "    if length > 0:\n",
    "        for j in range(length - 1):\n",
    "            # Update the transition probabilities\n",
    "            s_from = train_hs[i][j]\n",
    "            s_to = train_hs[i][j+1]\n",
    "            trans_prob[s_from][s_to] += 1\n",
    "\n",
    "for i in states:\n",
    "    for j in states:\n",
    "        # Normalize the frequency of the transition with the state counts\n",
    "        trans_prob[i][j] /= float(state_count[i])\n",
    "\n",
    "print(trans_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the emission probabilities\n",
    "emission_prob = {}\n",
    "\n",
    "# Get all the vocabulary in the corpus (train and test sets)\n",
    "vocab = list(set(train_wc.keys()) | set(test_wc.keys()))\n",
    "\n",
    "# Initialize the emission probabilities\n",
    "for state in states:\n",
    "    emission_prob[state] = {}\n",
    "    for word in vocab:\n",
    "        emission_prob[state][word] = 1\n",
    "\n",
    "for i in range(len(train_hs)):\n",
    "    length = len(train_hs[i])\n",
    "    for j in range(length):\n",
    "        # Update the emission probabilities\n",
    "        obs = train_data[i][j]\n",
    "        hidden = train_hs[i][j]\n",
    "        emission_prob[hidden][obs] += 1\n",
    "\n",
    "for state in states:\n",
    "    for word in vocab:\n",
    "        if emission_prob[state][word] == 0:\n",
    "            continue\n",
    "        else:\n",
    "            # Normalize the emission probabilities\n",
    "            emission_prob[state][word] /= float(state_count[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Viterbi algorithm below is used to find the most likely sequence of states.\n",
    "def viterbi_decoding(obs):\n",
    "    obs = [i for i in obs if i]\n",
    "    if len(obs) > 0:\n",
    "        # Initializing a list of dictionary to store the probabilites\n",
    "        V = [{}]\n",
    "        for st in states:\n",
    "            # Append the initial probabilites\n",
    "            V[0][st] = {\"prob\": start_prob[st] * emission_prob[st][obs[0]], \"prev\": None}\n",
    "        for t in range(1, len(obs)):\n",
    "            # Append a dictionary to store the probabilities at time/step t\n",
    "            V.append({})\n",
    "            for st in states:\n",
    "                max_tr_prob = V[t - 1][states[0]][\"prob\"] * trans_prob[states[0]][st]\n",
    "                prev_st_selected = states[0]\n",
    "                for prev_st in states[1:]:\n",
    "                    # Calculate the probabilities of each state\n",
    "                    tr_prob = V[t - 1][prev_st][\"prob\"] * trans_prob[prev_st][st]\n",
    "                    if tr_prob > max_tr_prob:\n",
    "                        max_tr_prob = tr_prob\n",
    "                        prev_st_selected = prev_st\n",
    "\n",
    "                # Get the max probability at time/step t\n",
    "                max_prob = max_tr_prob * emission_prob[st][obs[t]]\n",
    "                V[t][st] = {\"prob\": max_prob, \"prev\": prev_st_selected}\n",
    "\n",
    "        path = []\n",
    "        max_prob = -float(\"inf\")\n",
    "        best_st = None\n",
    "        # Get most probable state and its backtrack\n",
    "        for st, data in V[-1].items():\n",
    "            if data[\"prob\"] > max_prob:\n",
    "                max_prob = data[\"prob\"]\n",
    "                best_st = st\n",
    "        path.append(best_st)\n",
    "        previous = best_st\n",
    "\n",
    "        # Follow the backtrack till the first observation\n",
    "        for t in range(len(V) - 2, -1, -1):\n",
    "            path.insert(0, V[t + 1][previous][\"prev\"])\n",
    "            previous = V[t + 1][previous][\"prev\"]\n",
    "\n",
    "        # Return the path\n",
    "        return path\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the sentences of the testing data is then labelled based on the Viterbi algorithm\n",
    "test_pred = []\n",
    "for obs in test_data:\n",
    "    test_pred.append(viterbi_decoding(obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for state B: 0.6738491525681829\n",
      "F1 score for state M: 0.24104662322574127\n",
      "F1 score for state E: 0.6812865953626356\n",
      "F1 score for state S: 0.44521343198634034\n",
      "Macro-F1 score: 0.5103489507857251\n"
     ]
    }
   ],
   "source": [
    "# Based on the labels for the test data I calculate the F1 score to evaluate the model's performance.\n",
    "# I create a dictionary to store the mapping for each label\n",
    "map_dict = {\n",
    "    \"B\": 0,\n",
    "    \"M\": 1,\n",
    "    \"E\": 2,\n",
    "    \"S\": 3\n",
    "}\n",
    "\n",
    "# Flatten the lists into a 1D list\n",
    "true = list(np.concatenate(test_hs).flat)\n",
    "pred = list(np.concatenate(test_pred).flat)\n",
    "\n",
    "k = len(np.unique(true)) # Number of classes\n",
    "result = np.zeros((k, k)) # Initialize the confusion matrix\n",
    "\n",
    "for i in range(len(true)):\n",
    "    # Calculate the confusion matrix\n",
    "    result[map_dict[true[i]]][map_dict[pred[i]]] += 1\n",
    "\n",
    "# Precision for each label\n",
    "precision_B = result[0][0] / (result[0][0] + result[1][0] + result[2][0] + result[3][0])\n",
    "precision_M = result[1][1] / (result[0][1] + result[1][1] + result[2][1] + result[3][1])\n",
    "precision_E = result[2][2] / (result[0][2] + result[1][2] + result[2][2] + result[3][2])\n",
    "precision_S = result[3][3] / (result[0][3] + result[1][3] + result[2][3] + result[3][3])\n",
    "\n",
    "\n",
    "# Recall for each label\n",
    "recall_B = result[0][0] / (result[0][0] + result[0][1] + result[0][2] + result[0][3])\n",
    "recall_M = result[1][1] / (result[1][0] + result[1][1] + result[1][2] + result[1][3])\n",
    "recall_E = result[2][2] / (result[2][0] + result[2][1] + result[2][2] + result[2][3])\n",
    "recall_S = result[3][3] / (result[3][0] + result[3][1] + result[3][2] + result[3][3])\n",
    "\n",
    "\n",
    "# F1 for each label\n",
    "f1_B = 2 *(precision_B * recall_B)/(precision_B + recall_B)\n",
    "f1_M = 2 *(precision_M * recall_M)/(precision_M + recall_M)\n",
    "f1_E = 2 *(precision_E * recall_E)/(precision_E + recall_E)\n",
    "f1_S = 2 *(precision_S * recall_S)/(precision_S + recall_S)\n",
    "\n",
    "# Calculating Macro-F1\n",
    "macro_f1 = (f1_B + f1_M + f1_E + f1_S) / k\n",
    "\n",
    "print(f\"F1 score for state B: {f1_B}\")\n",
    "print(f\"F1 score for state M: {f1_M}\")\n",
    "print(f\"F1 score for state E: {f1_E}\")\n",
    "print(f\"F1 score for state S: {f1_S}\")\n",
    "print(f\"Macro-F1 score: {macro_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I apply the word breaking to the testing data\n",
    "def word_segmentation(test_data, pred):\n",
    "    segmented = \"\"\n",
    "    i = 0 # Counter for the test data index\n",
    "    j = 0 # Counter for the prediction index\n",
    "    while i < len(test_data):\n",
    "        segmented += test_data[i] \n",
    "        # Check for Chinese character\n",
    "        if test_data[i] > u\"\\u4e00\" and test_data[i] < u\"\\u9fff\":\n",
    "            # Add space after the character if label\n",
    "            # is either \"E\" or \"S\"\n",
    "            if pred[j] in [\"E\", \"S\"]:\n",
    "                segmented += \" \"\n",
    "            j += 1\n",
    "        i += 1\n",
    "    return segmented\n",
    "\n",
    "segmented = []\n",
    "for i in range(len(test_data)):\n",
    "    # Converting BMES label to word segmentation\n",
    "    segmented.append(word_segmentation(test_data[i], test_pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "国际  互联网络  的  发展  给  社会  带来  了  方便  和  巨大  的  商业  价值  ，  但  同时  也  产生  了  一些  副作用  。\n",
      "['B', 'E', 'B', 'E', 'B', 'E', 'S', 'B', 'E', 'S', 'B', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'S', 'B', 'E', 'B', 'E', 'S', 'B', 'E', 'S', 'B', 'E', 'S', 'B', 'E', 'S', 'B', 'E']\n",
      "国际 互联 网络 的 发展 给 社会 带来 了方 便和 巨大 的 商业 价值 但 同时 也 产生 了 一些副 作 用\n"
     ]
    }
   ],
   "source": [
    "# printing some random segmented sentence to see how it works\n",
    "i = np.random.randint(0, len(test_gold))\n",
    "print(test_gold[i])\n",
    "print(test_pred[i])\n",
    "print(segmented[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I save the file with segmentation as my_prediction.txt\n",
    "with open(\"my_prediction.txt\", \"w\", encoding=\"utf-8\") as fout:\n",
    "    for item in segmented:\n",
    "        fout.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toGw4XNnffsW"
   },
   "source": [
    "## tkinter GUI for HMM Machine Learning algorithm\n",
    "I am using tkinter to create a simple ui interface for the word segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "圆月当空月光如水世界一片朦胧\n",
      "圆月 当空 月光 如水 世界 一片 朦胧 \n",
      "我们不得不同心协力对付这些问题\n",
      "我们 不得 不同 心协 力对 付 这些 问题 \n",
      "我要出国学习英语但将  会  回来\n",
      "我要 出国 学习 英语 但将   会   回来 \n",
      "我要出国学习英语但将会回来\n",
      "我要 出国 学习 英语 但将 会 回来 \n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from tkinter.messagebox import showinfo\n",
    "\n",
    "# creating the window\n",
    "root = tk.Tk()\n",
    "root.geometry(\"300x150\")\n",
    "root.resizable(False, False)\n",
    "root.title('Word Segmentation with HMM')\n",
    "\n",
    "# First I create a string variable for storing the input text\n",
    "input_text = tk.StringVar()\n",
    "\n",
    "\n",
    "# get the text from the input field and use reverse matching before displaying the result in a message box when the segment button is clicked.\n",
    "def segment_clicked():\n",
    "    # callback for when the segment button is clicked\n",
    "\n",
    "    text = input_text.get()\n",
    "    print(text) # printing the input below to see it in jupyter notebook\n",
    "    #rm = reverse_matching('maximum_matching_dictionary.utf8')\n",
    "    print(word_segmentation(text, pred)) # printing the result below\n",
    "    msg = f'You entered the input: {input_text.get()} and the segmented text output is {word_segmentation(text, pred)}'\n",
    "    showinfo(\n",
    "        title='Result',\n",
    "        message=msg\n",
    "    )\n",
    "\n",
    "\n",
    "# input frame\n",
    "segmentation_frame = ttk.Frame(root)\n",
    "segmentation_frame.pack(padx=10, pady=10, fill='x', expand=True)\n",
    "\n",
    "\n",
    "# adding a label describing the input field\n",
    "input_label = ttk.Label(segmentation_frame, text=\"Input string:\")\n",
    "input_label.pack(fill='x', expand=True)\n",
    "\n",
    "# I create the Entry widget for the text input \n",
    "input_entry = ttk.Entry(segmentation_frame, textvariable=input_text)\n",
    "input_entry.pack(fill='x', expand=True)\n",
    "input_entry.focus()\n",
    "\n",
    "\n",
    "# segment button\n",
    "segment_button = ttk.Button(segmentation_frame, text=\"Segment text\", command=segment_clicked)\n",
    "segment_button.pack(fill='x', expand=True, pady=10)\n",
    "\n",
    "\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Chinese Word Segmentation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
